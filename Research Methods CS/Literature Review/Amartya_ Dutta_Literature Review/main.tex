\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{biblatex}

\title{\textbf{Literature Review}}
\author{Amartya Dutta}
\date{}


\usepackage[letterpaper, margin=1in]{geometry}
\addbibresource{ref.bib}

\begin{document}
\clearpage\maketitle
\thispagestyle{empty}
\section{Weakly supervised semantic segmentation}
Weakly supervised semantic segmentation (WS3) aims to train segmentation models using coarse-scale annotations that are not as precise as pixel-level locations of objects. In recent years, several WS3 methods have been proposed that use class labels to generate pseudo-ground truths for training segmentation models. Specifically, these methods employ localization maps, such as Class Activation Maps (CAMs) \cite{zhou2016learning, selvaraju2016grad}, which are generated from a pre-trained classifier, to guide the segmentation process. \newline
Recently a large body of work has focused on providing explanations of the model outputs which can potentially satisfy regulatory experiments, help practitioners debug their model and identify unintended bias in the model. This area of model explainability techniques can be categorized into two broad categories: (1) activation map based methods \cite{zhou2016learning, selvaraju2016grad}, (2) attribution map based methods \cite{simonyan2013deep, baehrens2010explain}.  \newline
\textbf{Activation maps} are feature maps from the last layer of CNNs which show regions in an image that are most responsible for the final prediction of the network. The activations are aggregated to generate a heatmap, highlighting the regions in the image that have the highest activations. One of the most popular activation map based methods is Class Activation Maps (CAMs) \cite{zhou2016learning}, where the activation map of the last CNN layer is combined with weights of the Global Average Pooling (GAP) layer to generate the visual explanations. Gradient-weighted Class Activation Mapping (Grad-CAM) \cite{selvaraju2016grad} is an extension of CAM that uses gradient information to improve the localization accuracy of the generated heatmaps. \textbf{Attribution maps} on the other hand are a form of interpretability technique that attempts to assign a score to each pixel in an image, representing its contribution to the final prediction of the network. There are various methods to compute attribution maps such as gradient-based methods. Gradient-based saliency map methods \cite{simonyan2013deep,baehrens2010explain} is the most common example of such a category, where the attribution map is generated by computing the gradient of the target class score w.r.t. the input image. The gradient quantifies how much change in the input pixel values corresponds to the change in target class scores. \newline
While Class Activation Maps (CAM) are good at highlighting discriminative regions (DR) of an image (i.e., regions that contribute significantly to the classifier’s decision), CAMs are known to disregard regions of the target object class that do not contribute to the classifier’s prediction, termed non-discriminative regions (NDR). This is because Class Activation Maps (CAMs) are essentially \textbf{activation maps} generated by the last convolutional neural network (CNN) layer, which are integrated with the weights of the final fully-connected layer. It has been shown that the final layer feature maps only contain information that is relevant to classification, a phenomenon called \textit{information bottleneck} \cite{lee2021reducing}. The RIB \cite{lee2021reducing} demonstrates that an information bottleneck occurs in later layers as only the task-relevant information is passed to the output. As a result, CAMs that are computed at the last layer have sparse coverage of the target object. Thus, CAMs are biased towards mostly finding DR while missing the NDR of the target object, which is equally important for the purpose of segmentation. A number of WS3 solutions thus require further processing of the CAM outputs to recover NDR for high segmentation accuracy \cite{li2018tell, hou2018self}. \newline
In contrast to activation maps, \textbf{attribution maps} provide an alternative approach for assigning a score to every pixel based on its contribution to the final neural network prediction. The most commonly used attribution map is the gradient-based Saliency Maps \cite{simonyan2013deep}. The basic idea of saliency is to calculate the gradient of the target class score with respect to every pixel in the input image. Attribution maps are fundamentally distinct from the activation maps obtained from the last layer of CNN models. However, despite the frequent use of attribution maps for interpretability purposes, their utility in WS3 has not yet been fully explored. 
\newpage
\printbibliography
\end{document}