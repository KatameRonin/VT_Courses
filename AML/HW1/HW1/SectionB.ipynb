{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cc8gZLT6WX7m"
   },
   "source": [
    "# CS-5824 / Advanced Machine Learning\n",
    "# Assignment 1 Section B [ 40 Points ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7isVWMS9WX7p"
   },
   "source": [
    "In this assignment, **you need to complete three sections** which are based on:\n",
    "\n",
    "1. Logistic regression\n",
    "2. MLE\n",
    "3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yxhzck1aWX7q"
   },
   "source": [
    "## Submission guideline\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your Virginia Tech PID below.\n",
    "3. Select Edit -> Clear All Output. This will clear all the outputs from all cells (but will keep the content of ll cells).\n",
    "4. Select Runtime -> Restart and Run All. This will run all the cells in order.\n",
    "5. Once you've rerun everything, select File -> Print -> Save as PDF\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. \n",
    "7. Upload **both** the PDF file and this notebook.\n",
    "8. Please **DO NOT** upload any data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NM7VD6NWX7r"
   },
   "source": [
    "### Please Write Your VT PID Here: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7vN9PE-ZFvg"
   },
   "source": [
    "# Section 0. Environment Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B8HcRjRUYktI"
   },
   "outputs": [],
   "source": [
    "!pip install scipy==1.1.0 Pillow==4.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Pja5zjFXwTp"
   },
   "source": [
    "Mount your google drive in google colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdVPU9EUXvij"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vg6RLG8AX73v"
   },
   "source": [
    "Append the directory to your python path using sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwSblWSIX-qM"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "prefix = '/content/gdrive/My Drive/'\n",
    "# modify \"customized_path_to_your_homework\" here to where you uploaded your homework\n",
    "customized_path_to_your_homework = 'CS 5824 ML/HW1/'\n",
    "sys_path = prefix + customized_path_to_your_homework\n",
    "sys.path.append(sys_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Shi_GW74rBGx"
   },
   "source": [
    "Run some setup code for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNkFUWI8WX7u"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook rather than in a new window.\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whfPbRDScOBs"
   },
   "source": [
    "## Section 1. Logistic Regression [ 18 points ]\n",
    "\n",
    "In this problem, we‚Äôll apply logistic regression to a data set of spam email. \n",
    "This data consists of 4601 email messages, from which 57 features have been extracted. These are as follows:\n",
    "- 48 features in [0, 100], giving the percentage of words in a given message which match a given\n",
    "word on a list containing, e.g., ‚Äúbusiness‚Äù, ‚Äúfree‚Äù, etc.\n",
    "- 6 features in [0, 100], giving the percentage of characters in the email that match characters on a\n",
    "list containing, e.g., ‚Äú$‚Äù, ‚Äú#‚Äù, etc.\n",
    "- Feature 55: The average length of an uninterrupted sequence of capital letters.\n",
    "- Feature 56: The length of the longest uninterrupted sequence of capital letters.\n",
    "- Feature 57: The sum of the lengths of uninterrupted sequences of capital letters.\n",
    "\n",
    "\n",
    "There are files spam.train.dat and spam.test.dat (provided in the assignment files) in which\n",
    "each row is an email. There are 3000 training and 1601 test examples. The final column in each file\n",
    "indicates whether the email was spam.\n",
    "\n",
    "The files can be loaded using the block of code below. You will answer the following questions using the data provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y88kiOiTcwKm"
   },
   "outputs": [],
   "source": [
    "prefix = '/content/gdrive/My Drive/'\n",
    "# modify \"customized_path_to_your_homework\" here to where your data is\n",
    "customized_path_to_your_homework = 'CS 5824 ML/HW1/'\n",
    "train_path = prefix + customized_path_to_your_homework + 'data/spam.train.dat'\n",
    "train_set = np.genfromtxt(data_path)\n",
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIM0eb1he8K2"
   },
   "source": [
    "### Question 1. [ 3 points ]\n",
    "\n",
    "Build a Logistic Regression model to classify whether an email is spam or not using the *spam* data set. Report your training and test performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iqd_VTJuf5jz"
   },
   "outputs": [],
   "source": [
    "#Build your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wy-OYYh9jigk"
   },
   "outputs": [],
   "source": [
    "#Report training and test performance here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APUOMYTGgHCv"
   },
   "source": [
    "### Question 2. [ 3 points ]\n",
    "\n",
    "Plot the learning curve for this model. What is the Bias/Variance trade-off for this model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Muh9NYKxgmMu"
   },
   "outputs": [],
   "source": [
    "#Plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVr3IIaUjrQk"
   },
   "outputs": [],
   "source": [
    "#Comment on the Bias/Variance trade-off "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOH01r2BgvMF"
   },
   "source": [
    "### Question 3. [ 3 points ]\n",
    "\n",
    "Apply L2-regularized logistic regression. Use cross-validation to determine an appropriate regularization penalty. Report your procedure and the value you find. What training and test performance do you get with this value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gupzh2L8g0xI"
   },
   "outputs": [],
   "source": [
    "#Apply L2-regularized logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDgIrxlIj4OP"
   },
   "outputs": [],
   "source": [
    "#Perform cross-validation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HkcnWrbFkL0V"
   },
   "outputs": [],
   "source": [
    "#Report your procedure and training and test performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwnfSzAhgv3D"
   },
   "source": [
    "### Question 4. [ 3 points ]\n",
    "\n",
    "Apply L1-regularized logistic regression. Use cross-validation to determine an appropriate regularization penalty. Report your procedure and the value you find. What training and test performance do you get with this value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isw6Jbm8jUGI"
   },
   "outputs": [],
   "source": [
    "#Apply L1-regularized logistic regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lG9ZiW1okgXv"
   },
   "outputs": [],
   "source": [
    "#Perform cross-validation \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ILi8jNTMki2z"
   },
   "outputs": [],
   "source": [
    "#Report your procedure and training and test performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEMnUbhNiKDv"
   },
   "source": [
    "### Question 5. [ 3 points ]\n",
    "\n",
    "What are the advantages and disadvantages of the two models with repect to this problem? For example, have there been a lot of sparceness in the model, or what kind of features have been removed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NqGkLu0kz_u"
   },
   "outputs": [],
   "source": [
    "# Advantages and disadvantages of the two models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmnlr_kTi2Rc"
   },
   "source": [
    "### Question 6. [3 points]\n",
    "\n",
    "Transform the features with the basis function of your choice. Retrain the two models above and report the model performances. Why did you choose this basis function ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1hqUonfi10i"
   },
   "outputs": [],
   "source": [
    "# Advantages and disadvantages of the two models \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RarKmxPMk7Eq"
   },
   "outputs": [],
   "source": [
    "#Retrain the two models above \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lsiLar7ulBz1"
   },
   "outputs": [],
   "source": [
    "#Report the model performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Why choosing this basis function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0Am_07CrKH_"
   },
   "source": [
    "# Section 2. MLE [15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTRlMoEkvcla"
   },
   "source": [
    "For the following problems, we will a collaborative filtering data set. These data\n",
    "are originally from http://eigentaste.berkeley.edu/dataset/, however, they have been altered somewhat for this HW, so you should use the files directly shared from us. These problems will only examine the marginal distribution of the ratings themselves. Assuming that the data set is in your homework path on google drive, the ratings can be loaded into your collab session using the block of code in the next cell. This will give you a 1761439 √ó 3 matrix of doubles. Right now we only care about the ratings, which are the third column. You‚Äôll be asked to produce figures. Include these figures in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QbQEPwxizLKa"
   },
   "outputs": [],
   "source": [
    "prefix = '/content/gdrive/My Drive/'\n",
    "# modify \"customized_path_to_your_homework\" here to where your data is\n",
    "customized_path_to_your_homework = 'CS 5824 ML/HW1/'\n",
    "data_path = prefix + customized_path_to_your_homework + 'data/jester_ratings.dat'\n",
    "data = np.genfromtxt(data_path)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0n6r6d8Nzpow"
   },
   "source": [
    "### Question 1. [ 5 points ]\n",
    "\n",
    "Generate a set of normalized histograms (histograms which have an area of one) of the ratings and\n",
    "qualitatively describe the empirical distributions that you see. Try several different bin sizes and explain your choices. Are the resulting density estimates uni- or multi-modal? Where do the peaks\n",
    "appear to be? Do these answers change as you vary the number of bins?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bo3cSrISzpow"
   },
   "outputs": [],
   "source": [
    "# Your answer here (code, histogram, response to questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsLbJtg1zpow"
   },
   "source": [
    "### Question 2. [ 5 points ]\n",
    "\n",
    "Perform a maximum-likelihood fit of a Gaussian distribution to the ratings and report the mean and\n",
    "variance. Overlay the MLE Gaussian fit on top of the normalized histogram. Is it a good fit or a bad fit\n",
    "and why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pYjgoOIwzpow"
   },
   "outputs": [],
   "source": [
    "# Fit MLE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report mean and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlay two histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5zJ0aObzpow"
   },
   "source": [
    "### Question 3. [ 5 points ]\n",
    "\n",
    "Randomly partition the data into ten disjoint sets (called folds) of approximately the same size. We will use these partitions to assess the generalization performance of these MLE fits. This is done by creating ten experiments where one fold is taken to be a ‚Äútest‚Äù set and the remaining nine are together considered to be the ‚Äútraining‚Äù set. A model is fit on the training data and asked to make predictions of the test set. For a given model, this produces ten log probability numbers that reflect how well the model generalized to the unseen data. If the folds are of different size, the predictive log probabilities can be turned into ‚Äúaverages‚Äù by dividing the overall logprob by the number of test cases. Perform this procedure for your Gaussian histogram density estimators from Problem 1 and 2. That is, fit this model ten times on 9/10ths of the data and ask it to make predictions of the remaining 1/10th. To visualize the results, produce a boxplot of the average log probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GT4yKbJUzpow"
   },
   "outputs": [],
   "source": [
    "# Perform 10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2C-Eupw4siz"
   },
   "source": [
    "## Section 3. Evaluation Questions  [7 points ]\n",
    "\n",
    "1. Suppose we fit a linear model to a polynomial data. Is this model a good fit? If not, is it underfitting or overfitting? [ 2 points ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kiae0v324JTt"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1dEQooh4BKm"
   },
   "source": [
    "2.  How does cross validation address the problem of overfitting? Does it only identify (or detect) overfitting? Does it also eliminate (or at least reduce) overfitting? Explain\n",
    "your answers. [2.5 points ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kiae0v324JTt"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "734sFMrm4Ecj"
   },
   "source": [
    "3. Suppose you are given a dataset { (ùë•<sub>1</sub>, ùë¶<sub>1</sub>), (ùë•<sub>2</sub>, ùë¶<sub>2</sub>), ‚ãØ , (ùë•<sub>m</sub>, ùë¶<sub>m</sub>) } and you are asked to perform **5-fold cross-validation** for selecting the value of ùúÜ for $ùêø_2$ regularization for a regularized linear regression. Please describe the procedure of how you would select the value ùúÜ. [2.5 points ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwfnZiSC4Kqs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
