{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CS-5824 / Advanced Machine Learning\n",
        "# Assignment 2 Section B [ 40 Points ]\n",
        "\n",
        "In this assignment, **you need to complete two sections** which are based on:\n",
        "\n",
        "1. Decision Trees (20 points)\n",
        "2. Support Vector Machines (20 points)"
      ],
      "metadata": {
        "id": "XG2Cj1xs36Qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission guidelines\n",
        "\n",
        "1. Click the Save button at the top of the notebook.\n",
        "2. Please make sure to enter your Virginia Tech PID below.\n",
        "3. Select Edit -> Clear All Output. This will clear all the outputs from all cells (but will keep the content of all cells).\n",
        "4. Select Runtime -> Restart and Run All. This will run all the cells in order.\n",
        "5. Once you've rerun everything, select File -> Print -> Save as PDF.\n",
        "6. Look at the PDF file and make sure all your solutions are there and correctly displayed. \n",
        "7. Upload **both** the PDF file (saved in step 5) and this notebook.\n",
        "8. Please **DO NOT** upload any data."
      ],
      "metadata": {
        "id": "FgSorA1E4NOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Your VT PID: "
      ],
      "metadata": {
        "id": "vzzlUwiX4QBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0. Environment Set Up"
      ],
      "metadata": {
        "id": "0zSO8tsI4S4L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount your Google Drive in Google Colab:"
      ],
      "metadata": {
        "id": "JpsAQ4Mf4Ynv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "metadata": {
        "id": "xFPTNy1u4lZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload all files in the zip to a directory in your Google Drive, then append it to your Python path using sys (please modify `customized_path_to_your_homework` to be the path to your directory):"
      ],
      "metadata": {
        "id": "vVAjLlqP4qZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "prefix = \"/content/gdrive/My Drive/\"\n",
        "customized_path_to_your_homework = \"CS 5824 ML/HW2/\"\n",
        "sys_path = prefix + customized_path_to_your_homework\n",
        "sys.path.append(sys_path)\n",
        "data_path = Path(sys_path) / \"Data\""
      ],
      "metadata": {
        "id": "3XtOebUS5LyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run some setup code for this notebook. For all randomization done in this assignment, please use the `seed` below as the random state."
      ],
      "metadata": {
        "id": "yYKsh3xT7H5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import warnings\n",
        "\n",
        "# We ignore the convergence warnings in this homework, as some of the exercise will\n",
        "# always trigger this warning.\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# This is a bit of magic to make matplotlib figures appear inline in the notebook rather than in a new window.\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams[\"image.interpolation\"] = \"nearest\"\n",
        "plt.rcParams[\"image.cmap\"] = \"gray\"\n",
        "\n",
        "# Some more magic so that the notebook will reload external python modules;\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Seed for all randomization\n",
        "seed = 5824"
      ],
      "metadata": {
        "id": "H4hwhmzo7S07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1. Decision Trees [ 15 points ]\n",
        "\n",
        "For this problem, we will use a decision tree classifier on a toy dataset provided by SciKit-Learn. We will experiment with the [Wine dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data). The data is the results of a chemical analysis of wines grown in the same region in Italy by three different cultivators. There are thirteen different measurements taken for different constituents found in the three types of wine. The features include:\n",
        "\n",
        "<ol>\n",
        "  <li>Alcohol</li>\n",
        "  <li>Malic acid</li>\n",
        "  <li>Ash</li>\n",
        "  <li>Alcalinity of ash</li>\n",
        "  <li>Magnesium</li>\n",
        "  <li>Total phenols</li>\n",
        "  <li>Flavanoids</li>\n",
        "  <li>Nonflavanoid phenols</li>\n",
        "  <li>Proanthocyanins</li>\n",
        "  <li>Color intensity</li>\n",
        "  <li>Hue</li>\n",
        "  <li>OD280/OD315 of diluted wines</li>\n",
        "  <li>Proline</li>\n",
        "</ol>\n",
        "\n",
        "The 3 classes are `class_0`, `class_1`, and `class_2`."
      ],
      "metadata": {
        "id": "yYigFuw57YdL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Data Preparation\n",
        "\n",
        "First, we need to load the dataset from SciKit-Learn. The `load_wine()` function returns a `sklearn.utils.Bunch` object containing all information about the dataset, such as feature and target names, as well as the full description of the data in its `DESCR` property."
      ],
      "metadata": {
        "id": "WdbQOafOX2Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "from sklearn.datasets import load_wine\n",
        "\n",
        "wine_dataset = load_wine()\n",
        "feature_names = wine_dataset.feature_names\n",
        "target_names = wine_dataset.target_names\n",
        "wine_X, wine_y = wine_dataset.data, wine_dataset.target\n",
        "print(wine_dataset.DESCR)"
      ],
      "metadata": {
        "id": "-2xjz-I1X9U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize the data, then split it into train (80%) and test (20%) sets using the provided `seed` and with stratification:"
      ],
      "metadata": {
        "id": "uT4STLRLYElT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Preprocess data\n",
        "wine_X_train, wine_X_test, wine_y_train, wine_y_test = None, None, None, None"
      ],
      "metadata": {
        "id": "7fPsQP9IcaIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For ease of visualization, we will be using only the first 2 features (\"Alcohol\" and \"Malic acid\"). Visualize the train set below:"
      ],
      "metadata": {
        "id": "dHiucEoFdhtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colors = [\"tab:red\", \"tab:olive\", \"tab:blue\"]\n",
        "feature1, feature2 = 0, 1 # Chosen features\n",
        "for label, color in zip(range(len(target_names)), colors):\n",
        "    idx = np.where(wine_y_train == label)\n",
        "    plt.scatter(\n",
        "        wine_X_train[idx, feature1],\n",
        "        wine_X_train[idx, feature2],\n",
        "        s=100,\n",
        "        color=color,\n",
        "        edgecolor=\"black\",\n",
        "        label=target_names[label]\n",
        "    )\n",
        "plt.xlabel(target_names[0])\n",
        "plt.ylabel(target_names[1])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w0MQX-ZPYrYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Training a Decision Tree (5 points)\n",
        "\n",
        "Using [`sklearn.tree.DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), train a decision tree classifier with entropy as the purity criterion, the provided `seed` as the random state, and a maximum depth of 3."
      ],
      "metadata": {
        "id": "3gt_y67pitlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train your decision tree"
      ],
      "metadata": {
        "id": "aipe6EcyZPiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot your decision tree as a graph:"
      ],
      "metadata": {
        "id": "u-qhi1l3uRvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "plot_tree(tree_clf)"
      ],
      "metadata": {
        "id": "B65XX3fsudAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also plot the decision boundaries using the `sklearn.inspection.DecisionBoundaryDisplay` class—in particular, its `from_estimator()` method—which is [new in SciKit-Learn 1.1.2](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.DecisionBoundaryDisplay.html#sklearn.inspection.DecisionBoundaryDisplay). Due to package availability reasons, we include it in the attached `utils.py` file."
      ],
      "metadata": {
        "id": "uUYFVJeBujtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils import DecisionBoundaryDisplay\n",
        "\n",
        "def plot_decision_boundary(clf, X, y, chosen_features, feature_names, target_names):\n",
        "    feature1, feature2 = chosen_features\n",
        "    fig, ax = plt.subplots()\n",
        "    DecisionBoundaryDisplay.from_estimator(\n",
        "        clf,\n",
        "        np.column_stack((X[:, feature1], X[:, feature2])),\n",
        "        cmap=plt.cm.RdYlBu,\n",
        "        response_method=\"predict\",\n",
        "        ax=ax,\n",
        "    )\n",
        "\n",
        "    for label, color in zip(range(len(target_names)), colors):\n",
        "        idx = np.where(y == label)\n",
        "        plt.scatter(\n",
        "            X[idx, feature1],\n",
        "            X[idx, feature2],\n",
        "            s=100,\n",
        "            color=color,\n",
        "            edgecolor=\"black\",\n",
        "            label=target_names[label]\n",
        "        )\n",
        "\n",
        "    plt.xlabel(feature_names[feature1])\n",
        "    plt.ylabel(feature_names[feature2])\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "plot_decision_boundary(\n",
        "    tree_clf,\n",
        "    wine_X_train,\n",
        "    wine_y_train,\n",
        "    (feature1, feature2),\n",
        "    feature_names,\n",
        "    target_names\n",
        ")"
      ],
      "metadata": {
        "id": "8aiLMEYLabGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Predicting (10 points)\n",
        "\n",
        "There are two different functions for prediction within `DecisionTreeClassifier`. "
      ],
      "metadata": {
        "id": "lR-Qq_6wuNNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) What are they? Invoke them on the test set in the cells below and look at the outputs. How are they different? How are they related? (2 points)\n",
        "\n",
        "**_Your answer:_**"
      ],
      "metadata": {
        "id": "6HCrLs6EFvX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Predict function 1"
      ],
      "metadata": {
        "id": "dT_yWp6hGEzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Predict function 2"
      ],
      "metadata": {
        "id": "EKLJBPCxFOfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Compute the accuracy, precision, and F1-score to assess your decision tree's performance below. How is the performance? (3 points)\n",
        "\n",
        "**_Your answer:_**"
      ],
      "metadata": {
        "id": "FoecOj0cGvXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Evaluate decision tree's performance"
      ],
      "metadata": {
        "id": "45L_10_yHfa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) Experiment with different parameters (*e.g.*, depth, selection criterion) and observe the change in decision boundaries as well as the evaluation scores. Report the optimal configuration and its corresponding scores. How does changing the depth improve or worsen the performance of the decision tree, specifically in the context of this dataset? (5 points)\n",
        "\n",
        "**_Your answer:_**"
      ],
      "metadata": {
        "id": "FmhiXrRNIOXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2. Support Vector Machines [ 25 points ]\n",
        "\n",
        "In this section, you will experiment with different datasets using SciKit-Learn's implementation of support vector machines (SVMs). This will give you better intuitions about utilizing SVMs when it comes to different two-dimensional datasets. You will also implement a Gaussian kernel for non-linear SVM classification.\n",
        "\n",
        "## 2.1. Linear SVM (5 points)\n",
        "\n",
        "In this subsection, we will experiment with a dataset that can be separated with a linear decision boundary. We want to experiment with different $C$ values to understand their effects on our linear decision boundary.\n",
        "\n",
        "Let's first load the dataset (`svm_data1.mat` in MatLab format) using SciPy's `loadmat()` function."
      ],
      "metadata": {
        "id": "WKPwwUBIycPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = loadmat(data_path / \"svm_data1.mat\")\n",
        "X1, y1 = data1[\"X\"], data1[\"y\"][:, 0] # \"X\" and \"y\" are keys for this dataset\n",
        "X1.shape, y1.shape # 2D samples + labels"
      ],
      "metadata": {
        "id": "aeKMzLGb8n23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We provide code for plotting data and SVM decision boundaries below:"
      ],
      "metadata": {
        "id": "S2bwDxBmBkVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(X, y, ax=None):\n",
        "    \"\"\"Plot 2D dataset.\"\"\"\n",
        "    positive = (y == 1)\n",
        "    negative = (y == 0)\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    ax.plot(X[positive, 0], X[positive, 1], \"X\", mew=1, ms=10, mec=\"k\")\n",
        "    ax.plot(X[negative, 0], X[negative, 1], \"o\", mew=1, mfc=\"tab:olive\", ms=10, mec=\"k\")\n",
        "\n",
        "\n",
        "def plot_linear_boundary(X, y, model, ax=None):\n",
        "    \"\"\"Plot linear boundary.\"\"\"\n",
        "    if model is None:\n",
        "        return\n",
        "    w = model.coef_[0]    # the theta of your SVM classifier\n",
        "    b = model.intercept_  # the bias of your SVM classifier\n",
        "    xp = np.array([np.min(X[:, 0]), np.max(X[:, 0])])\n",
        "    yp = -(w[0] * xp + b) / w[1]\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    plot_data(X, y, ax)\n",
        "    ax.plot(xp, yp)\n",
        "    \n",
        "    \n",
        "def plot_nonlinear_boundary(X, y, model, ax=None):\n",
        "    \"\"\"Contour plot that delineates a nonlinear boundary.\"\"\"\n",
        "    if model is None:\n",
        "        return\n",
        "    num_points = X.shape[0]\n",
        "    x1 = np.linspace(min(X[:, 0]), max(X[:, 0]), num_points)\n",
        "    x2 = np.linspace(min(X[:, 1]), max(X[:, 1]), num_points)\n",
        "    X1, X2 = np.meshgrid(x1, x2)\n",
        "    vals = np.zeros(X1.shape)\n",
        "    for i in range(X1.shape[1]):\n",
        "        X_ = np.stack((X1[:, i], X2[:, i]), axis=1)\n",
        "        vals[:, i] = model.predict(X_)\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "    ax.contourf(X1, X2, vals, cmap=\"YlGnBu\", alpha=0.2)    \n",
        "    plot_data(X, y, ax)"
      ],
      "metadata": {
        "id": "RxYuqTAgm-QP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to first visualize our dataset in two-dimensional space:"
      ],
      "metadata": {
        "id": "urnRBVEAO64I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training data\n",
        "plot_data(X1, y1)"
      ],
      "metadata": {
        "id": "UATxrqtUm_Wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Describe the dataset. How does it lend itself well to the use of an SVM? Are there any abnormalities that may affect our model's performance? (2 points)\n",
        "\n",
        "**_Your answer:_**"
      ],
      "metadata": {
        "id": "683vIH5VONpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we want to train linear SVMs for our dataset. You should pick **6 different $C$ values** to train your SVMs, then plot all of them side by side for convenient juxtaposition. You should train your models with **L2 penalty** and **hinge loss**. Use the provided `seed`. We provide code for side-by-side plots below."
      ],
      "metadata": {
        "id": "83Vxq-RUoffi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# TODO: Replace None's with appropriate values\n",
        "Cs = [None, None, None, None, None, None]\n",
        "\n",
        "cols = 3\n",
        "rows = math.ceil(len(Cs) / cols)\n",
        "\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(15, 8))\n",
        "for row in range(rows):\n",
        "    for col in range(cols):\n",
        "        C = Cs[row * cols + col]\n",
        "        # TODO: Train SVM\n",
        "        svm_clf = None\n",
        "        # End of code\n",
        "        plot_linear_boundary(X1, y1, svm_clf, axes[row, col])\n",
        "        axes[row, col].set_title(f\"C = {C}\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "LM2Lxeq4oo98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) What does $C$ intuitively represent, and how does varying it affect our SVM's decision boundary, according to the graphs? Take into account any abnormalities mentioned above. (3 points)\n",
        "\n",
        "**_Your answer:_**"
      ],
      "metadata": {
        "id": "U1FbNjmrowzs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Kernel SVM (20 points)\n",
        "\n",
        "In this part of the homework, you will perform non-linear classification using SVMs, with Gaussian kernels in particular.\n",
        "\n",
        "### 2.2.1. Gaussian Kernel (10 points)\n",
        "\n",
        "Similarly to Gaussian basis functions, we can use Gaussian kernels to find non-linear decision boundaries. As mentioned in lectures and chapter 6.2 of the Bishop textbook (page 296), these kernels are of the form:\n",
        "\n",
        "$$\n",
        "    k(\\mathbf{x}, \\mathbf{x}') = \\exp \\left\\{ -\\frac{\\lVert\\mathbf{x} - \\mathbf{x}'\\rVert^2}{2\\sigma^2}\\right\\}\n",
        "$$"
      ],
      "metadata": {
        "id": "ysLYgE7To7pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a Gaussian kernel matrix for our SVMs below. We will be using [`sklearn.svm.SVC`](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) as our model."
      ],
      "metadata": {
        "id": "WZ8NIMVAo96R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gaussian_kernel(x, x_prime, sigma):\n",
        "    k = None\n",
        "    # TODO: Implement Gaussian kernel matrix\n",
        "    # End of code\n",
        "    return k"
      ],
      "metadata": {
        "id": "xb--ZovMpQ5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that according to [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), our resulting kernel matrix should have size `(n_samples, n_samples)`. Once you have completed the implemetation of `gaussian_kernel_matrix()`, the following cell will test your kernel function on two provided examples, after which you should expect to see a value of:\n",
        "\n",
        "<pre>\n",
        "[[1.         0.32465247]\n",
        " [0.32465247 1.        ]]\n",
        "</pre>"
      ],
      "metadata": {
        "id": "9iNnev7MpYat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = np.array([[1, 2, 1],[0, 4, -1]])\n",
        "x2 = np.array([[1, 2, 1],[0, 4, -1]])\n",
        "sigma = 2.0\n",
        "\n",
        "kernel_matrix = gaussian_kernel(x1, x2, sigma)\n",
        "print(kernel_matrix)"
      ],
      "metadata": {
        "id": "q3xEvSSmpZkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2. Training SVM with a Gaussian Kernel (5 points)\n",
        "\n",
        "Let's demonstrate a Gaussian-kernel SVM on a non-linear dataset:"
      ],
      "metadata": {
        "id": "YqzakRwapcwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = loadmat(data_path / \"svm_data2.mat\")\n",
        "X2, y2 = data2[\"X\"], data2[\"y\"][:, 0]\n",
        "plot_data(X2, y2)"
      ],
      "metadata": {
        "id": "nH2cAJaopfCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply a Gaussian kernel to your non-linear SVC model below; an SVC example with a custom kernel is available [here](https://scikit-learn.org/stable/auto_examples/svm/plot_custom_kernel.html). Note the kernel's input arguments."
      ],
      "metadata": {
        "id": "Y-hIg-lDRS6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigma = 0.1\n",
        "# TODO: Apply Gaussian kernel on SVC with provided seed as random_state\n",
        "model = None\n",
        "# End of code\n",
        "plot_nonlinear_boundary(X2, y2, model) # note that this step could take up to 1 minute"
      ],
      "metadata": {
        "id": "izolTlEupqS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.3. Grid Search Cross Validation (5 points)\n",
        "\n",
        "In this part of the homework, you will utilize cross validation with a validation set to finetune your model. You will use Gaussian-kernel SVMs for this task. From the provided dataset (`svm_data3.mat`), you are given a train and validation set:"
      ],
      "metadata": {
        "id": "65rLSWOItJSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data3 = loadmat(data_path / \"svm_data3.mat\")\n",
        "\n",
        "X_train = data3[\"X\"]\n",
        "y_train = data3[\"y\"][:, 0]\n",
        "X_val = data3[\"Xval\"]\n",
        "y_val = data3[\"yval\"][:, 0]\n",
        "\n",
        "# Plot training and validation data\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "plot_data(X_train, y_train, ax1)\n",
        "plot_data(X_val, y_val, ax2)\n",
        "ax1.set_title(\"Train set\")\n",
        "ax2.set_title(\"Validation set\")"
      ],
      "metadata": {
        "id": "FgGCi5vMtKqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you are to use the validation set (`X_val`, `y_val`) to determine the best combination of $C$ and $\\sigma$ parameters for your SVM, using accuracy as your metric. We suggest trying values in multiplicative steps for these parameters (e.g., 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30). You should try all possible pairs of values for $C$ and $\\sigma$. For example, trying every possible combination of the 2 parameters among the 8 values listed above would result in $8^2 = 64$ different models being trained and validated. Because this could take a long time, we suggest reserving time to work on this section.\n",
        "\n",
        "Write code to determine the best combination of $C$ and $\\sigma$ as well as to return the corresponding values in `search_hyperparameter()` below."
      ],
      "metadata": {
        "id": "9tolp91wtM1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_hyperparameter(X_train, y_train, X_val, y_val, Cs, sigmas):\n",
        "    # TODO: Grid search\n",
        "    best_C = None\n",
        "    best_sigma = None\n",
        "    best_accuracy = None\n",
        "    return best_C, best_sigma, best_accuracy"
      ],
      "metadata": {
        "id": "IZUItMCAtURR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report the best performing parameters along with the accuracy score on `X_val`. You should be able to get an accuracy higher than 0.9. The optimal parameters may not be unique."
      ],
      "metadata": {
        "id": "Kk-xyAlQtWaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Cs = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n",
        "sigmas = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n",
        "\n",
        "# TODO: Try different SVM hyperparameters"
      ],
      "metadata": {
        "id": "a7f_epBDtaui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, train the model again with your optimal parameters and plot the decision boundary."
      ],
      "metadata": {
        "id": "omRx58Mptcnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train and plot model with best configuration"
      ],
      "metadata": {
        "id": "GiUkz0yutiOZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}